---
layout: post
title: "Hadoop数据压缩"
author: Haokang Mu
excerpt: Hadoop数据压缩.md
tags:
- Hadoop
- MapReduce

---



# 1 概述

### 1）压缩的好处和坏处

压缩的优点：以减少磁盘IO、减少磁盘存储空间。

压缩的缺点：增加CPU开销。

### 2）压缩原则

（1）运算密集型的Job，少用压缩

（2）IO密集型的Job，多用压缩

# 2 MR支持的压缩编码

### 1）压缩算法对比介绍

![image](https://user-images.githubusercontent.com/65494322/141740339-05e096a4-9624-4931-bbc4-750a1cf3354e.png)


### 2）压缩性能的比较

![image](https://user-images.githubusercontent.com/65494322/141740390-07e6f1ab-63bc-48ef-8abf-728bc67fe800.png)


http://google.github.io/snappy/

Snappy is a compression/decompression library. It **does not aim for maximum compression**, or compatibility with any other compression library; instead, **it aims for very high speeds** and reasonable compression. For instance, compared to the fastest mode of zlib, Snappy is an order of magnitude faster for most inputs, but the resulting compressed files are anywhere from 20% to 100% bigger.On a single core of a Core i7 processor in 64-bit mode, **Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.**

# 3 压缩方式选择

压缩方式选择时重点考虑：**压缩/解压缩速度、压缩率（压缩后存储大小）、压缩后是否可以支持切片。**

## 3.1 Gzip压缩

优点：压缩率比较高； 

缺点：不支持Split；压缩/解压速度一般；

## 3.2 Bzip2压缩

优点：压缩率高；支持Split； 

缺点：压缩/解压速度慢。

## 3.3 Lzo压缩

优点：压缩/解压速度比较快；支持Split；

缺点：压缩率一般；想支持切片需要额外创建索引。

## 3.4 Snappy压缩

优点：压缩和解压缩速度快； 

缺点：不支持Split；压缩率一般； 

## 3.5 压缩位置选择
压缩可以在MapReduce作用的任意阶段启用。

#### Map输入端采用压缩

在有大量数据并计划重复处理的情况下，应考虑对输入进行压缩。无须显示指定使用的编解码方式。Hadoop自动检查文件扩展名，根据扩展名自动选择编码方式对文件进行压缩和解压。（bzip2支持分片）

企业开发：考虑因素

1）数据量小于块大小，重点考虑压缩和解压缩速度比较快的LZO/Snappy

2）数据量非常大，重点考虑支持切片的Bzip2和LZO

#### Map输出端采用压缩

当map任务输出的中间数据量很大时。

能提升Shuffle性能。

可选择LZO或者Snappy.

企业开发中如何选择：为了减少MapTask和ReduceTask之间的网络IO。重点考虑压缩和解压缩快的LZO、Snappy。

#### Reducer输出采用压缩

看需求：

    如果数据永久保存，考虑压缩率比较高的Bzip2和Gzip。
    
    如果作为下一个MapReduce输入，需要考虑数据量和是否支持切片。

# 4 压缩参数配置

### 1）为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器

![image](https://user-images.githubusercontent.com/65494322/141740473-9e5b41f5-f06f-4263-9780-fb2577b3f105.png)


### 2）要在Hadoop中启用压缩，可以配置如下参数

![image](https://user-images.githubusercontent.com/65494322/141740509-8329597b-cad0-4409-9cce-8617c7cd7dda.png)




# 5 压缩实操案例

## 5.1 Map输出端采用压缩

即使你的MapReduce的输入输出文件都是未压缩的文件，你仍然可以对Map任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到Reduce节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可

### 1）Hadoop源码支持的压缩格式有：BZip2Codec、DefaultCodec

```
import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.compress.BZip2Codec;	
import org.apache.hadoop.io.compress.CompressionCodec;
import org.apache.hadoop.io.compress.GzipCodec;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCountDriver {

	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {

		Configuration conf = new Configuration();

		// 开启map端输出压缩
		conf.setBoolean("mapreduce.map.output.compress", true);

		// 设置map端输出压缩方式
		conf.setClass("mapreduce.map.output.compress.codec", BZip2Codec.class,CompressionCodec.class);

		Job job = Job.getInstance(conf);

		job.setJarByClass(WordCountDriver.class);

		job.setMapperClass(WordCountMapper.class);
		job.setReducerClass(WordCountReducer.class);

		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(IntWritable.class);

		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);

		FileInputFormat.setInputPaths(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));

		boolean result = job.waitForCompletion(true);

		System.exit(result ? 0 : 1);
	}
}
```

### 2）Mapper保持不变

```
import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable>{

	Text k = new Text();
	IntWritable v = new IntWritable(1);

	@Override
	protected void map(LongWritable key, Text value, Context context)throws IOException, InterruptedException {

		// 1 获取一行
		String line = value.toString();

		// 2 切割
		String[] words = line.split(" ");

		// 3 循环写出
		for(String word:words){
			k.set(word);
			context.write(k, v);
		}
	}
}
```

### 3）Reducer保持不变

```
import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable>{

	IntWritable v = new IntWritable();

	@Override
	protected void reduce(Text key, Iterable<IntWritable> values,
			Context context) throws IOException, InterruptedException {
		
		int sum = 0;

		// 1 汇总
		for(IntWritable value:values){
			sum += value.get();
		}
		
         v.set(sum);

         // 2 输出
		context.write(key, v);
	}
}
```

## 5.2 Reduce输出端采用压缩

### 1）修改驱动

```
import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.compress.BZip2Codec;
import org.apache.hadoop.io.compress.DefaultCodec;
import org.apache.hadoop.io.compress.GzipCodec;
import org.apache.hadoop.io.compress.Lz4Codec;
import org.apache.hadoop.io.compress.SnappyCodec;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCountDriver {

	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
		
		Configuration conf = new Configuration();
		
		Job job = Job.getInstance(conf);
		
		job.setJarByClass(WordCountDriver.class);
		
		job.setMapperClass(WordCountMapper.class);
		job.setReducerClass(WordCountReducer.class);
		
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(IntWritable.class);
		
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		
		FileInputFormat.setInputPaths(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		
		// 设置reduce端输出压缩开启
		FileOutputFormat.setCompressOutput(job, true);

		// 设置压缩的方式
	    FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class); 
//	    FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class); 
//	    FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class); 
	    
		boolean result = job.waitForCompletion(true);
		
		System.exit(result?0:1);
	}
}
```

### 2）Mapper和Reducer保持不变（详见上）
