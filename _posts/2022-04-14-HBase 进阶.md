---
layout: post
title: "HBase进阶"
author: Haokang Mu
excerpt: HBase进阶.md
tags:
- HBase

---

# 1、架构原理
![image.png](https://cdn.nlark.com/yuque/0/2022/png/25452040/1649765666046-fa2b9bc8-792d-4a3b-a3fd-7b58fd57c2d7.png#clientId=u0a08b26d-cf7c-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=461&id=ud74a502f&margin=%5Bobject%20Object%5D&name=image.png&originHeight=461&originWidth=957&originalType=binary&ratio=1&rotation=0&showTitle=false&size=298242&status=done&style=none&taskId=uc2999ec5-2cd9-4189-9671-6190fc0fd2c&title=&width=957)
**1）StoreFile **
保存实际数据的物理文件，StoreFile 以 HFile 的形式存储在 HDFS 上。每个 Store 会有一个或多个 StoreFile（HFile），数据在每个 StoreFile 中都是有序的。 

**2）MemStore **
写缓存，由于 HFile 中的数据要求是有序的，所以数据是先存储在 MemStore 中，排好序后，等到达刷写时机才会刷写到 HFile，每次刷写都会形成一个新的 HFile。

**3）WAL **
由于数据要经 MemStore 排序后才能刷写到 HFile，但把数据保存在内存中会有很高的 概率导致数据丢失，为了解决这个问题，数据会先写在一个叫做 Write-Ahead logfile 的文件 中，然后再写入 MemStore 中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。

# 2、写流程
![image.png](https://cdn.nlark.com/yuque/0/2022/png/25452040/1649765983537-01f92d16-5171-454a-b0b9-23cce07bda2b.png#clientId=u0a08b26d-cf7c-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=472&id=u7379b4bb&margin=%5Bobject%20Object%5D&name=image.png&originHeight=472&originWidth=956&originalType=binary&ratio=1&rotation=0&showTitle=false&size=248236&status=done&style=none&taskId=u90f2b004-cceb-4acc-a5c5-90dc6d32a93&title=&width=956)
写流程： 
1）Client 先访问 zookeeper，获取 hbase:meta 表位于哪个 Region Server。 
```shell
[zk: localhost:2181(CONNECTED) 1] ls /
[admin, brokers, cluster, config, consumers, controller_epoch, hbase, isr_change_notification, latest_producer_id_block, locks, servers, spark, zookeeper]
[zk: localhost:2181(CONNECTED) 2] ls /hbase
[backup-masters, draining, flush-table-proc, hbaseid, master, meta-region-server, namespace, online-snapshot, recovering-regions, region-in-transition, replication, rs, running, splitWAL, switch, table, table-lock]
[zk: localhost:2181(CONNECTED) 3] get /hbase/meta-region-server
�regionserver:16020�}6��PBUF

        hadoop104�}���ˁ0
```
![image.png](https://cdn.nlark.com/yuque/0/2022/png/25452040/1649833360415-062d67e3-e1ce-4a3e-9c30-86f3d1e40a37.png#clientId=u6889a29a-5fa5-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=203&id=u569ed608&margin=%5Bobject%20Object%5D&name=image.png&originHeight=283&originWidth=863&originalType=binary&ratio=1&rotation=0&showTitle=false&size=49087&status=done&style=none&taskId=uf705c4bc-6960-4d83-b24d-0144c3ee450&title=&width=618)

2）访问对应的 Region Server，获取 hbase:meta 表，根据读请求的 namespace:table/rowkey， 查询出目标数据位于哪个 Region Server 中的哪个 Region 中。并将该 table 的 region 信息以及 meta 表的位置信息缓存在客户端的 meta cache，方便下次访问。 
![image.png](https://cdn.nlark.com/yuque/0/2022/png/25452040/1649833629399-ba8a4804-a9f4-4507-81b4-8c98923a780c.png#clientId=u6889a29a-5fa5-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=196&id=u0cb7c7f2&margin=%5Bobject%20Object%5D&name=image.png&originHeight=196&originWidth=833&originalType=binary&ratio=1&rotation=0&showTitle=false&size=207398&status=done&style=none&taskId=u9bb408f3-9add-4bba-ad2a-d9a1cda1cdb&title=&width=833)
扫描hbase:meta这张表，发现student表是由hadoop102这台机器上的Region Server维护的

3）与目标 Region Server 进行通讯； 

4）将数据顺序写入（追加）到 WAL； 

5）将数据写入对应的 MemStore，数据会在 MemStore 进行排序； 

6）向客户端发送 ack； 

7）等达到 MemStore 的刷写时机后，将数据刷写到 HFile。

# 3、MemStore Flush
![image.png](https://cdn.nlark.com/yuque/0/2022/png/25452040/1649766087232-1f6c5670-209a-4202-b15b-ee3de2b0aded.png#clientId=u0a08b26d-cf7c-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=439&id=u0f0894e8&margin=%5Bobject%20Object%5D&name=image.png&originHeight=439&originWidth=645&originalType=binary&ratio=1&rotation=0&showTitle=false&size=129392&status=done&style=none&taskId=u86a98e9f-2df7-4c7f-adda-3d8c6b2d211&title=&width=645)   
**MemStore 刷写时机： **
1、当某个 memstroe 的大小达到了 **hbase.hregion.memstore.flush.size（默认值 128M）**，
其所在 region 的所有 memstore 都会刷写。 
当 memstore 的大小达到了 
**hbase.hregion.memstore.flush.size（默认值 128M）* hbase.hregion.memstore.block.multiplier（默认值 4） **时，会阻止继续往该 memstore 写数据。 

2、当 region server 中 memstore 的总大小达到 
**java_heapsize *hbase.regionserver.global.memstore.size（默认值 0.4）*hbase.regionserver.global.memstore.size.lower.limit（默认值 0.95）**， 
region 会按照其所有 memstore 的大小顺序（由大到小）依次进行刷写。直到 region server 中所有 memstore 的总大小减小到上述值以下。 

当 region server 中 memstore 的总大小达到 
**java_heapsize*hbase.regionserver.global.memstore.size（默认值 0.4） **
时，会阻止继续往所有的 memstore 写数据。 

3、到达自动刷写的时间，也会触发 memstore flush。自动刷新的时间间隔由该属性进行配置 **hbase.regionserver.optionalcacheflushinterval（默认 1 小时）**。

4、当 WAL 文件的数量超过 **hbase.regionserver.max.logs**，region 会按照时间顺序依次进 行刷写，直到 WAL 文件数量减小到 **hbase.regionserver.max.log **以下（该属性名已经废弃，现无需手动设置，最大值为 32）。

# 4、读流程
![image.png](https://cdn.nlark.com/yuque/0/2022/png/25452040/1649768302818-9d3401a4-9bc7-43ae-872d-3c34d4e60c34.png#clientId=u0a08b26d-cf7c-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=461&id=u005da675&margin=%5Bobject%20Object%5D&name=image.png&originHeight=461&originWidth=955&originalType=binary&ratio=1&rotation=0&showTitle=false&size=215961&status=done&style=none&taskId=u4a210d9f-2ced-4f18-908b-68291530c81&title=&width=955)
**读流程 **
1）Client 先访问 zookeeper，获取 hbase:meta 表位于哪个 Region Server。 

2）访问对应的 Region Server，获取 hbase:meta 表，根据读请求的 namespace:table/rowkey， 查询出目标数据位于哪个 Region Server 中的哪个 Region 中。并将该 table 的 region 信息以及 meta 表的位置信息缓存在客户端的 meta cache，方便下次访问。 

3）与目标 Region Server 进行通讯； 

4）分别在 Block Cache（读缓存），MemStore 和 Store File（HFile）中查询目标数据，并将查到的所有数据进行合并。此处所有数据是指同一条数据的不同版本（time stamp）或者不同的类型（Put/Delete）。 

5） 将从文件中查询到的数据块（Block，HFile 数据存储单元，默认大小为 64KB）缓存到 
Block Cache。 

6）将合并后的最终结果返回给客户端。

# 5、StoreFile Compaction 
由于memstore每次刷写都会生成一个新的HFile，且同一个字段的不同版本（timestamp） 和不同类型（Put/Delete）有可能会分布在不同的 HFile 中，因此查询时需要遍历所有的 HFile。为了减少 HFile 的个数，以及清理掉过期和删除的数据，会进行 StoreFile Compaction。 

Compaction 分为两种，分别是 Minor Compaction 和 Major Compaction。Minor Compaction 会将临近的若干个较小的 HFile 合并成一个较大的 HFile，但**不会**清理过期和删除的数据。 
Major Compaction 会将一个 Store 下的所有的 HFile 合并成一个大 HFile，并且**会**清理掉过期 
和删除的数据。 
![image.png](https://cdn.nlark.com/yuque/0/2022/png/25452040/1649769906795-1acd2ab2-1002-4ee0-91b4-18670378b55c.png#clientId=u0a08b26d-cf7c-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=488&id=u4fa04373&margin=%5Bobject%20Object%5D&name=image.png&originHeight=488&originWidth=949&originalType=binary&ratio=1&rotation=0&showTitle=false&size=203571&status=done&style=none&taskId=uec985e30-988e-4909-9f4a-fdd9352ae65&title=&width=949)

# 6、Region Split 
默认情况下，每个 Table 起初只有一个 Region，随着数据的不断写入，Region 会自动进行拆分。刚拆分时，两个子 Region 都位于当前的 Region Server，但处于负载均衡的考虑，HMaster 有可能会将某个 Region 转移给其他的 Region Server。 

Region Split 时机： 
1、当1个region中的某个Store下所有StoreFile的总大小超过hbase.hregion.max.filesize， 该 Region 就会进行拆分（0.94 版本之前）。 

2、当 1 个 region 中 的 某 个 Store 下所有 StoreFile 的 总 大 小 超 过 Min(R^2 * "hbase.hregion.memstore.flush.size",hbase.hregion.max.filesize")，该 Region 就会进行拆分，其 
中 R 为当前 Region Server 中属于该 Table 的个数（0.94 版本之后）。
![image.png](https://cdn.nlark.com/yuque/0/2022/png/25452040/1649770070047-d1ae27aa-9b02-40a0-a9d4-84adb7865940.png#clientId=u0a08b26d-cf7c-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=438&id=u945894e0&margin=%5Bobject%20Object%5D&name=image.png&originHeight=438&originWidth=935&originalType=binary&ratio=1&rotation=0&showTitle=false&size=253216&status=done&style=none&taskId=u4f37a973-e6cb-493f-be3e-7b29488cf65&title=&width=935)

