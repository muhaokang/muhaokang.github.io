# 1. Spark 是什么
![image.png](https://cdn.nlark.com/yuque/0/2022/png/25452040/1647485071114-a935679b-2db1-4f37-9db6-d2db82db0079.png#clientId=ua8b8ad9a-c0d7-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=279&id=ud64c080d&margin=%5Bobject%20Object%5D&name=image.png&originHeight=279&originWidth=956&originalType=binary&ratio=1&rotation=0&showTitle=false&size=88923&status=done&style=none&taskId=ua15e29f5-b719-476d-bc7a-86a5c77eb91&title=&width=956)
Spark 是一种基于内存的快速、通用、可扩展的大数据分析计算引擎。
# 2. Spark and Hadoop
在之前的学习中，Hadoop 的MapReduce 是大家广为熟知的计算框架，那为什么咱们还要学习新的计算框架Spark 呢，这里就不得不提到Spark 和Hadoop 的关系。

首先从时间节点上来看:
 Hadoop

- 2006 年1 月，Doug Cutting 加入Yahoo，领导Hadoop 的开发
- 2008 年1 月，Hadoop 成为Apache 顶级项目
- 2011 年1.0 正式发布
- 2012 年3 月稳定版发布
- 2013 年10 月发布2.X (Yarn)版本

 Spark

- 2009 年，Spark 诞生于伯克利大学的AMPLab 实验室
- 2010 年，伯克利大学正式开源了Spark 项目
- 2013 年6 月，Spark 成为了Apache 基金会下的项目
- 2014 年2 月，Spark 以飞快的速度成为了Apache 的顶级项目
- 2015 年至今，Spark 变得愈发火爆，大量的国内公司开始重点部署或者使用Spark

然后我们再从功能上来看:
Hadoop

- Hadoop 是由java 语言编写的，在分布式服务器集群上存储海量数据并运行分布式分析应用的开源框架
- 作为Hadoop 分布式文件系统，HDFS 处于Hadoop 生态圈的最下层，存储着所有的数据，支持着Hadoop 的所有服务。它的理论基础源于Google 的TheGoogleFileSystem 这篇论文，它是GFS 的开源实现。
- MapReduce 是一种编程模型，Hadoop 根据Google 的MapReduce 论文将其实现， 作为Hadoop 的分布式计算模型，是Hadoop 的核心。基于这个框架，分布式并行程序的编写变得异常简单。综合了HDFS 的分布式存储和MapReduce 的分布式计算，Hadoop 在处理海量数据时，性能横向扩展变得非常容易。
- HBase 是对Google 的Bigtable 的开源实现，但又和Bigtable 存在许多不同之处。HBase 是一个基于HDFS 的分布式数据库，擅长实时地随机读/写超大规模数据集。它也是Hadoop 非常重要的组件。

Spark

- Spark 是一种由Scala 语言开发的快速、通用、可扩展的大数据分析引擎
- Spark Core 中提供了Spark 最基础与最核心的功能
- Spark SQL 是Spark 用来操作结构化数据的组件。通过Spark SQL，用户可以使用SQL 或者Apache Hive 版本的SQL 方言（HQL）来查询数据。
- Spark Streaming 是Spark 平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API。

由上面的信息可以获知，Spark 出现的时间相对较晚，并且主要功能主要是用于数据计算，所以其实Spark 一直被认为是Hadoop 框架的升级版。

# 3. Spark or Hadoop
Hadoop 的MR 框架和Spark 框架都是数据处理框架，那么我们在使用时如何选择呢？

- Hadoop MapReduce 由于其设计初衷并不是为了满足循环迭代式数据流处理，因此在多并行运行的数据可复用场景（如：机器学习、图挖掘算法、交互式数据挖掘算法）中存在诸多计算效率等问题。所以Spark 应运而生，Spark 就是在传统的MapReduce 计算框架的基础上，利用其计算过程的优化，从而大大加快了数据分析、挖掘的运行和读写速度，并将计算单元缩小到更适合并行计算和重复使用的RDD 计算模型。
- 机器学习中ALS、凸优化梯度下降等。这些都需要基于数据集或者数据集的衍生数据反复查询反复操作。MR 这种模式不太合适，即使多MR 串行处理，性能和时间也是一个问题。数据的共享依赖于磁盘。另外一种是交互式数据挖掘，MR 显然不擅长。而Spark 所基于的scala 语言恰恰擅长函数的处理。
- Spark 是一个分布式数据快速分析项目。它的核心技术是弹性分布式数据集（Resilient Distributed Datasets），提供了比MapReduce 丰富的模型，可以快速在内存中对数据集进行多次迭代，来支持复杂的数据挖掘算法和图形计算算法。
- Spark 和Hadoop 的根本差异是多个作业之间的数据通信问题: Spark 多个作业之间数据通信是基于内存，而Hadoop 是基于磁盘。
- Spark  Task 的启动时间快。Spark 采用fork 线程的方式，而Hadoop 采用创建新的进程的方式。
- Spark 只有在shuffle 的时候将数据写入磁盘，而Hadoop 中多个MR 作业之间的数据交互都要依赖于磁盘交互
- Spark 的缓存机制比HDFS 的缓存机制高效。

经过上面的比较，我们可以看出在绝大多数的数据计算场景中，Spark 确实会比MapReduce 更有优势。但是Spark 是基于内存的，所以在实际的生产环境中，由于内存的限制，可能会由于内存资源不够导致Job 执行失败，此时，MapReduce 其实是一个更好的选择，所以Spark 并不能完全替代MR。
# 4. Spark 核心模块
![image.png](https://cdn.nlark.com/yuque/0/2022/png/25452040/1647485479226-3038479c-9f0a-4b09-998a-e8fd5e0e63bb.png#clientId=ua8b8ad9a-c0d7-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=528&id=u51f5b46f&margin=%5Bobject%20Object%5D&name=image.png&originHeight=528&originWidth=1280&originalType=binary&ratio=1&rotation=0&showTitle=false&size=2031401&status=done&style=none&taskId=u168cf6c6-8c5e-4a61-900d-18766aa801c&title=&width=1280)
### Spark Core
Spark Core 中提供了Spark 最基础与最核心的功能，Spark 其他的功能如：Spark SQL，Spark Streaming，GraphX, MLlib 都是在Spark Core 的基础上进行扩展的
### Spark SQL
Spark SQL 是Spark 用来操作结构化数据的组件。通过Spark SQL，用户可以使用SQL
或者Apache Hive 版本的SQL 方言（HQL）来查询数据。
### Spark Streaming
Spark Streaming 是Spark 平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API。
### Spark MLlib
MLlib 是Spark 提供的一个机器学习算法库。MLlib 不仅提供了模型评估、数据导入等额外的功能，还提供了一些更底层的机器学习原语。
### Spark GraphX
GraphX 是Spark 面向图计算提供的框架与算法库。

