---
layout: post
title: "k8s核心技术--Pod"
author: Haokang Mu
excerpt: k8s核心技术--Pod.md
tags:
- k8s

---


# 1. Pod概述
Pod是K8S系统中可以创建和管理的最小单元，是资源对象模型中由用户创建或部署的最小资源对象模型，也是在K8S上运行容器化应用的资源对象，其它的资源对象都是用来支撑或者扩展Pod对象功能的，比如控制器对象是用来管控Pod对象的，Service或者Ingress资源对象是用来暴露Pod引用对象的，PersistentVolume资源对象是用来为Pod提供存储等等，K8S不会直接处理容器，而是Pod，Pod是由一个或多个container组成。

Pod是Kubernetes的最重要概念，每一个Pod都有一个特殊的被称为 “根容器”的Pause容器。Pause容器对应的镜像属于Kubernetes平台的一部分，除了Pause容器，每个Pod还包含一个或多个紧密相关的用户业务容器。
![image.png](https://cdn.nlark.com/yuque/0/2022/png/25452040/1644825082699-0d16792a-85ec-4454-920d-dcf1216eaf35.png#clientId=u32a82065-aa2d-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=548&id=u93b1aa6e&margin=%5Bobject%20Object%5D&name=image.png&originHeight=548&originWidth=961&originalType=binary&ratio=1&rotation=0&showTitle=false&size=104025&status=done&style=none&taskId=u21485ac6-2345-4c6d-a854-afd16bdb3fe&title=&width=961)
## 1.1 Pod基本概念

- 最小部署的单元
- Pod里面是由一个或多个容器组成【一组容器的集合】
- 一个pod中的容器是共享网络命名空间
- Pod是短暂的
- 每个Pod包含一个或多个紧密相关的用户业务容器
## 1.2 Pod存在的意义

- 创建容器使用docker，一个docker对应一个容器，一个容器有进程，一个容器运行一个应用程序，docker没有pod的概念，一个docker就是一个容器一个进程
- Pod是多进程设计，运用多个应用程序，也就是一个Pod里面有多个容器，而一个容器里面运行一个应用程序
- Pod的存在是为了亲密性应用
   - 两个应用之间进行交互
   - 网络之间的调用【通过127.0.0.1 或 socket】
   - 两个应用之间需要频繁调用


Pod是在K8S集群中运行部署应用或服务的最小单元，它是可以支持多容器的。Pod的设计理念是支持多个容器在一个Pod中共享网络地址和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。同时Pod对多容器的支持是K8S中最基础的设计理念。在生产环境中，通常是由不同的团队各自开发构建自己的容器镜像，在部署的时候组合成一个微服务对外提供服务。

Pod是K8S集群中所有业务类型的基础，可以把Pod看作运行在K8S集群上的小机器人，不同类型的业务就需要不同类型的小机器人去执行。目前K8S的业务主要可以分为以下几种

- 长期伺服型：long-running
- 批处理型：batch
- 节点后台支撑型：node-daemon
- 有状态应用型：stateful application

上述的几种类型，分别对应的小机器人控制器为：Deployment、Job、DaemonSet 和 StatefulSet (后面将介绍控制器)
# 2. Pod实现机制
主要有以下两大机制

- 共享网络
- 共享存储
## 2.1 共享网络
容器本身之间相互隔离的，一般是通过 **namespace** 和 **group** 进行隔离，那么Pod里面的容器如何实现通信？

- 首先需要满足前提条件，也就是容器都在同一个**namespace**之间

关于Pod实现原理，首先会在Pod会创建一个根容器： pause容器，然后我们在创建业务容器 【nginx，redis 等】，在我们创建业务容器的时候，会把它添加到 info容器 中
而在 info容器 中会独立出 ip地址，mac地址，port 等信息，然后实现网络的共享
![image.png](https://cdn.nlark.com/yuque/0/2022/png/25452040/1644826473602-4dc3d9e2-c909-43bb-9bb6-031c6e8607ef.png#clientId=u6d9a3bca-5eab-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=523&id=ua820de13&margin=%5Bobject%20Object%5D&name=image.png&originHeight=523&originWidth=826&originalType=binary&ratio=1&rotation=0&showTitle=false&size=83242&status=done&style=none&taskId=ue89c7ec6-8a6a-4fad-8f7c-7fc81510697&title=&width=826)

- 通过 Pause 容器，把其它业务容器加入到Pause容器里，让所有业务容器在同一个名称空间中，可以实现网络共享

## 2.2 共享存储
Pod持久化数据，专门存储到某个地方中
![image.png](https://cdn.nlark.com/yuque/0/2022/png/25452040/1644826539838-4b5b788c-67b9-47fa-a9ec-a3e141236eab.png#clientId=u6d9a3bca-5eab-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=457&id=u00a1f483&margin=%5Bobject%20Object%5D&name=image.png&originHeight=457&originWidth=1232&originalType=binary&ratio=1&rotation=0&showTitle=false&size=90158&status=done&style=none&taskId=u247aa6f9-26d9-4f70-9b2c-ba19f70c339&title=&width=1232)
使用 Volumn数据卷进行共享存储，案例如下所示
![image.png](https://cdn.nlark.com/yuque/0/2022/png/25452040/1644826562317-7b481133-b4c9-479c-a0e0-585c193285b5.png#clientId=u6d9a3bca-5eab-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=561&id=ub0e7f9d4&margin=%5Bobject%20Object%5D&name=image.png&originHeight=561&originWidth=1231&originalType=binary&ratio=1&rotation=0&showTitle=false&size=108846&status=done&style=none&taskId=u20c31f5a-c58c-42c7-94c9-cde4726ca2b&title=&width=1231)
# 3. 镜像拉取策略
我们以具体实例来说，拉取策略就是 imagePullPolicy
![image.png](https://cdn.nlark.com/yuque/0/2022/png/25452040/1644826626767-13ef1b48-52c8-4260-a9f6-924c846b7b5b.png#clientId=u6d9a3bca-5eab-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=282&id=ue70ba720&margin=%5Bobject%20Object%5D&name=image.png&originHeight=282&originWidth=576&originalType=binary&ratio=1&rotation=0&showTitle=false&size=52063&status=done&style=none&taskId=u0a815860-b75f-44d7-8476-58eb7e23f30&title=&width=576)
拉取策略主要分为了以下几种

- IfNotPresent：默认值，镜像在宿主机上不存在才拉取
- Always：每次创建Pod都会重新拉取一次镜像
- Never：Pod永远不会主动拉取这个镜像


**说明：**
在生产环境中部署容器时，你应该避免使用 :latest 标签，因为这使得正在运行的镜像的版本难以追踪，并且难以正确地回滚。
相反，应指定一个有意义的标签，如 v1.42.0。

# 4. Pod资源限制
也就是我们Pod在进行调度的时候，可以对调度的资源进行限制，例如我们限制 Pod调度是使用的资源是2C4G，那么在调度对应的node节点时，只会占用对应的资源，对于不满足资源的节点，将不会进行调度
![image.png](https://cdn.nlark.com/yuque/0/2022/png/25452040/1644826967796-dd93cf35-733b-4de7-b0bb-390049cf236d.png#clientId=u6d9a3bca-5eab-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=514&id=u72e9556a&margin=%5Bobject%20Object%5D&name=image.png&originHeight=514&originWidth=1106&originalType=binary&ratio=1&rotation=0&showTitle=false&size=96726&status=done&style=none&taskId=u144ef601-1a6d-479e-878a-c78e8793704&title=&width=1106)
**示例**
我们在下面的地方进行资源的限制
![image.png](https://cdn.nlark.com/yuque/0/2022/png/25452040/1644827026502-d8e28fb4-3f08-4c36-a308-73542a14aab9.png#clientId=u6d9a3bca-5eab-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=530&id=ufbd35cbe&margin=%5Bobject%20Object%5D&name=image.png&originHeight=530&originWidth=484&originalType=binary&ratio=1&rotation=0&showTitle=false&size=63740&status=done&style=none&taskId=u3a091bef-7f12-4ef0-b070-58bf6fffff2&title=&width=484)
这里分了两个部分

- request：表示调度所需的资源
- limits：表示最大所占用的资源
# 5. Pod重启机制
因为Pod中包含了很多个容器，假设某个容器出现问题了，那么就会触发Pod重启机制
![image.png](https://cdn.nlark.com/yuque/0/2022/png/25452040/1644827081131-e4d9932c-d634-45e4-a149-dd4e7dbdc080.png#clientId=u6d9a3bca-5eab-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=373&id=u577b5a60&margin=%5Bobject%20Object%5D&name=image.png&originHeight=373&originWidth=612&originalType=binary&ratio=1&rotation=0&showTitle=false&size=53659&status=done&style=none&taskId=ued4e8db7-4cd0-407a-bf35-5235796ba8e&title=&width=612)
重启策略主要分为以下三种

- Always：当容器终止退出后，总是重启容器，默认策略 【nginx等，需要不断提供服务】
- OnFailure：当容器异常退出（退出状态码非0）时，才重启容器。
- Never：当容器终止退出，从不重启容器 【批量任务】
# 6. Pod健康检查
通过容器检查，原来我们使用下面的命令来检查
```shell
[root@k8s-master ~]# kubectl get pod
NAME                    READY   STATUS    RESTARTS   AGE
nginx-f89759699-glcl2   1/1     Running   0          46h
web-66bf4959f5-lw8tx    1/1     Running   0          19h
```
但是有的时候，程序可能出现了 **Java** 堆内存溢出，程序还在运行，但是不能对外提供服务了，这个时候就不能通过 容器检查来判断服务是否可用了

这个时候就可以使用应用层面的检查
```shell
# 存活检查，如果检查失败，将杀死容器，根据Pod的restartPolicy【重启策略】来操作
livenessProbe

# 就绪检查，如果检查失败，Kubernetes会把Pod从Service endpoints中剔除
readinessProbe
```
![image.png](https://cdn.nlark.com/yuque/0/2022/png/25452040/1644827250492-8d37ca7d-2217-4d0a-970b-cc068fe84cb9.png#clientId=u6d9a3bca-5eab-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=610&id=u4d67c72b&margin=%5Bobject%20Object%5D&name=image.png&originHeight=610&originWidth=529&originalType=binary&ratio=1&rotation=0&showTitle=false&size=84351&status=done&style=none&taskId=u4ac870d3-28ee-46cc-9c51-32e9300d0bc&title=&width=529)
Probe支持以下三种检查方式

- http Get：发送HTTP请求，返回200 - 400 范围状态码为成功
- exec：执行Shell命令返回状态码是0为成功
- tcpSocket：发起TCP Socket建立成功

# 7. Pod调度策略
## 7.1 创建Pod流程

- 首先创建一个pod，然后进入到API Server进行pod的相关操作，并且把过程进入到Etcd里进行存储 【把创建出来的信息存储在etcd中】createpod--API Server--etcd
- 然后 Scheduler，监控API Server是否有新的Pod，如果有的话，通过etcd读取这个pod，通过调度算法，把pod调度某个node上 
- 在node节点，会通过kubelet--API Server读取etcd 拿到分配在当前node节点上的pod，然后通过docker创建容器

![image.png](https://cdn.nlark.com/yuque/0/2022/png/25452040/1644827455715-654811b7-5728-4e2f-925a-f30fe496d484.png#clientId=u6d9a3bca-5eab-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=651&id=udfa45f77&margin=%5Bobject%20Object%5D&name=image.png&originHeight=651&originWidth=1146&originalType=binary&ratio=1&rotation=0&showTitle=false&size=182439&status=done&style=none&taskId=ue5c400d1-a7b6-4de9-bf9d-38c3f83eadc&title=&width=1146)
## 7.2 影响Pod调度的属性
Pod资源限制对Pod的调度会有影响
### 7.2.1 根据request找的足够node节点进行调度
![image.png](https://cdn.nlark.com/yuque/0/2022/png/25452040/1644828322192-9df9a34e-c9a7-454a-8832-b83a62425380.png#clientId=u6d9a3bca-5eab-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=530&id=ub6e49541&margin=%5Bobject%20Object%5D&name=image.png&originHeight=530&originWidth=484&originalType=binary&ratio=1&rotation=0&showTitle=false&size=63740&status=done&style=none&taskId=u45570bb1-e216-4417-84e9-dfdc561e4e2&title=&width=484)
### 7.2.2 节点选择器标签影响Pod调度
![image.png](https://cdn.nlark.com/yuque/0/2022/png/25452040/1644828369638-07e07064-3c39-4bf5-8845-dd92159f4a88.png#clientId=u6d9a3bca-5eab-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=333&id=u83a93367&margin=%5Bobject%20Object%5D&name=image.png&originHeight=333&originWidth=482&originalType=binary&ratio=1&rotation=0&showTitle=false&size=126163&status=done&style=none&taskId=u10901e46-2104-4c00-bf0e-f3abf6b61ac&title=&width=482)
关于节点选择器，其实就是环境不同，然后环境之间所用的资源配置不同
![image.png](https://cdn.nlark.com/yuque/0/2022/png/25452040/1644828421340-49db00c9-ed9b-494d-8715-3080a71eeb83.png#clientId=u6d9a3bca-5eab-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=481&id=uf9793e5f&margin=%5Bobject%20Object%5D&name=image.png&originHeight=481&originWidth=1276&originalType=binary&ratio=1&rotation=0&showTitle=false&size=70507&status=done&style=none&taskId=u22d2dc35-a1fa-4bfc-a295-9b7e682d5b6&title=&width=1276)
我们可以通过以下命令，给我们的节点新增标签，然后节点选择器就会进行调度了
```shell
kubectl label node node1 env_role=prod
```
### 7.3 节点亲和性
节点亲和性 **nodeAffinity** 和 之前nodeSelector 基本一样的，根据节点上标签约束来决定Pod调度到哪些节点上，功能更加强大

- 硬亲和性：约束条件必须满足，不满足一直是等待状态
- 软亲和性：尝试满足，不保证绝对满足

![image.png](https://cdn.nlark.com/yuque/0/2022/png/25452040/1644828589025-e933861c-5912-446c-a913-1f2f12fe063c.png#clientId=u6d9a3bca-5eab-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=632&id=u154647c4&margin=%5Bobject%20Object%5D&name=image.png&originHeight=632&originWidth=1341&originalType=binary&ratio=1&rotation=0&showTitle=false&size=292512&status=done&style=none&taskId=u3cb967d7-0150-4ea6-964b-917c66b1731&title=&width=1341)
支持常用操作符：in、NotIn、Exists、Gt、Lt、DoesNotExists

- 反亲和性：就是和亲和性刚刚相反，如 NotIn、DoesNotExists等

# 8. 污点和污点容忍
## 8.1 污点
### 8.1.1 污点概述
nodeSelector 和 NodeAffinity，Pod调度到某些节点上，属于Pod的属性，是在调度的时候实现的。
Taint 污点：节点不做普通分配调度，是节点属性
### 8.1.2 场景

- 专用节点【限制ip】
- 配置特定硬件的节点【固态硬盘】
- 基于Taint驱逐【在node1不放，在node2放】
### 8.1.3 查看污点情况
```shell
[root@k8s-master ~]# kubectl describe node k8s-master | grep Taint
Taints:             node.kubernetes.io/unreachable:NoExecute
```
污点值有三个

- NoSchedule：一定不被调度
- PreferNoSchedule：尽量不被调度【也有被调度的几率】
- NoExecute：不会调度，并且还会驱逐Node已有Pod
### 8.1.4 为节点添加污点
```shell
kubectl taint node [node] key=value:污点的三个值
```
举例
```shell
[root@k8s-master ~]# kubectl taint node k8s-node1 env_role=yes:NoSchedule
node/k8s-node1 tainted
[root@k8s-master ~]# kubectl describe node k8s-node1 | grep Taint
Taints:             env_role=yes:NoSchedule
```
### 8.1.5 删除污点
```shell
[root@k8s-master ~]# kubectl taint node k8s-node1 env_role:NoSchedule-
node/k8s-node1 untainted
[root@k8s-master ~]# kubectl describe node k8s-node1 | grep Taint
Taints:             <none>
```
### 8.1.6 演示
我们现在创建多个Pod，查看最后分配到Node上的情况
1、首先我们创建一个 nginx 的pod
```shell
[root@k8s-master ~]# kubectl create deployment web --image=nginx
deployment.apps/web created
[root@k8s-master ~]# kubectl get pods -o wide
NAME                   READY   STATUS              RESTARTS   AGE   IP       NODE        NOMINATED NODE   READINESS GATES
web-5dcb957ccc-mzsr9   0/1     ContainerCreating   0          21s   <none>   k8s-node1   <none>           <none>
```
我们可以非常明显的看到，这个Pod已经被分配到 k8snode1 节点上了
2、下面我把pod复制5份，在查看情况pod情况
```shell
[root@k8s-master ~]# kubectl scale deployment web --replicas=5
deployment.apps/web scaled
[root@k8s-master ~]# kubectl get pods -o wide
NAME                   READY   STATUS              RESTARTS   AGE   IP           NODE        NOMINATED NODE   READINESS GATES
web-5dcb957ccc-4pdn2   0/1     ContainerCreating   0          5s    <none>       k8s-node2   <none>           <none>
web-5dcb957ccc-5k7dp   0/1     ContainerCreating   0          5s    <none>       k8s-node1   <none>           <none>
web-5dcb957ccc-mzsr9   1/1     Running             0          85s   10.244.1.6   k8s-node1   <none>           <none>
web-5dcb957ccc-prqhf   0/1     ContainerCreating   0          5s    <none>       k8s-node2   <none>           <none>
web-5dcb957ccc-v9p29   0/1     ContainerCreating   0          5s    <none>       k8s-node2   <none>           <none>
```
我们可以发现，因为master节点存在污点的情况，所以节点都被分配到了 node1 和 node2节点上
3、我们可以使用下面命令，把刚刚我们创建的pod都删除
```shell
[root@k8s-master ~]# kubectl delete deployment web
deployment.apps "web" deleted
[root@k8s-master ~]# kubectl get pods -o wide
No resources found in default namespace.
```
4、现在给了更好的演示污点的用法，我们现在给 node1节点打上污点，然后我们查看污点是否成功添加
```shell
[root@k8s-master ~]# kubectl taint node k8s-node1 env_role=yes:NoSchedule
node/k8s-node1 tainted
[root@k8s-master ~]# kubectl describe node k8s-node1 | grep Taint
Taints:             env_role=yes:NoSchedule
```
5、然后我们在创建一个 pod
```shell
[root@k8s-master ~]# kubectl create deployment web --image=nginx
deployment.apps/web created
[root@k8s-master ~]# kubectl scale deployment web --replicas=5
deployment.apps/web scaled
```
6、我们能够看到现在所有的pod都被分配到了 k8s-node2上，因为刚刚我们给node1节点设置了污点
```shell
[root@k8s-master ~]# kubectl get pods -o wide
NAME                   READY   STATUS              RESTARTS   AGE   IP           NODE        NOMINATED NODE   READINESS GATES
web-5dcb957ccc-7px52   0/1     ContainerCreating   0          46s   <none>       k8s-node2   <none>           <none>
web-5dcb957ccc-gsw56   1/1     Running             0          50s   10.244.2.7   k8s-node2   <none>           <none>
web-5dcb957ccc-n2mpw   0/1     ContainerCreating   0          46s   <none>       k8s-node2   <none>           <none>
web-5dcb957ccc-n7f5x   1/1     Running             0          46s   10.244.2.8   k8s-node2   <none>           <none>
web-5dcb957ccc-vdt7z   1/1     Running             0          46s   10.244.2.9   k8s-node2   <none>           <none>
```
7、最后我们可以删除刚刚添加的污点
```shell
[root@k8s-master ~]# kubectl taint node k8s-node1 env_role:NoSchedule-
node/k8s-node1 untainted
```
## 8.2 污点容忍
污点容忍就是某个节点可能被调度，也可能不被调度，相当于软亲和性的效果，就算设置了NoSchedule，也可能会被调度
![image.png](https://cdn.nlark.com/yuque/0/2022/png/25452040/1644830349116-bf39b3a1-7f34-4743-8e3d-efa9258e4251.png#clientId=u6d9a3bca-5eab-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=264&id=u412ad4ad&margin=%5Bobject%20Object%5D&name=image.png&originHeight=264&originWidth=406&originalType=binary&ratio=1&rotation=0&showTitle=false&size=62461&status=done&style=none&taskId=u8d6147b8-d7a4-46b6-8f5f-7c80c9b7dc3&title=&width=406)
